% !TeX root = ../thuthesis-example.tex

\chapter{引言}

卷积神经网络现在被广泛应用在生活的各个方面，比如可以刷脸的门禁系统，目标的定位检测系统以及图像搜索系统等等。
这些系统的模型都是由用户产生的个人数据训练的，其中有一些数据涉及到个人的隐私。
那么这些模型的信息在泄露以后是否能被有心人利用，从而还原出个人的隐私信息呢？
本章将从卷积神经网络应用的普及，机器学习模型的攻击，GDPR等隐私法规的出台等方面介绍本文的选题背景。最后再介绍本文选题的意义。


\section{研究背景及意义}

\subsection{当今卷积神经网络的主要应用}
应用一：图像的分类和检索。
图像的分类就是输入图片以后，输出这张图片在若干指定类别中最有可能是的类别，常使用监督式的机器学习方法。在使用卷积神经网络以前，常用的监督式机器学习有k近邻聚类算法（K-nearest-neighbours，KNN），主成分分析法（Principle Component Analysis, PCA）以及支持向量机分类方法(Support Vector Machine, SVM)【1】。这些传统机器学习方法的一个共同点是需要提前提取特征。而卷积神经网络则不用，卷积操作会自动捕捉图像中的模式。特征的自动提取使得机器学习收到了广泛的欢迎，也使得卷积神经网络被科研工作者和企业工程师广泛接受。卷积神经网络因此也常常用作特征提取器。利用卷积神经网络进行图像分类典型的应用场景是图像搜索。【2】这个发明就提出了一种利用卷积神经网络进行快速图片检索的方法，并且把卷积神经网络当作特征提取器来使用，再结合其他基于特征距离的分类方法实现快速的图片检索。
应用二：目标的定位、检测和图像的分割。
目标定位的任务是标出图片中物体的位置。目标定位和图像分类都有检测物体类别的属性，它们的不同点是目标定位除了要判别图片中有什么物体之外，还要给出物体在图片中的位置，一般用方框来标记物体位置。
目标检测的任务是识别图片中不定数量物体以及他们各自所在的位置。目标检测与目标定位的不同之处在与目标定位中物体的数量是固定的，而目标检测中物体的数量是不固定的。就是说目标检测需要把图片中所有目标物体全都识别出来并且准确地给出它们的位置。
图像分割的任务是在像素级别圈出图片中的目标物体。图像分割与目标检测不同的地方是图像分割是在像素级别将物体圈出，而目标检测则不需要这么精细，只需要用方框圈出大致的范围即可。如图一所示，图一用图形展示了图形分类、目标定位、目标检测和图像分割的不同之处。
目标定位和目标检测的典型应用场景是自动驾驶、安全防护和医疗领域。图像分割的典型应用场景有视频的后期制作，图像处理软件如Photoshop、美图秀秀等。
应用三：人脸识别。
人脸识别顾名思义就是在图像或视频中识别出人脸的技术。和图像分类技术类似，早期人脸识别技术采用的是传统机器学习的方法进行的。其过程一般是通过摄像头获取图像或视频，然后通过计算机对图片或视频进行预处理，通过选取关键点来提取图片特征，再将提取出来的图片特征与训练好的模型进行比对，从而决定图片的分类，最终将分类结果输出。这种方式仍然避免不了人工选取关键点来提取图片特征的环节。以卷积神经网络技术为代表的深度学习技术结束了手工选择关键点的过程，直接可以通过卷积神经网络就能自动提取相关特征，从而达到理想的分类效果。2014年以DeepFace为代表的利用深度神经网络进行人脸识别的工作相继出现，使得人脸识别准确率不断提高。2015年FaceNet在LFW数据集上达到99.67 $\%$的准确率，宣布人工智能首度超过人类的人脸识别能力。随着人脸识别技术的不断成熟，其应用场景也不断增多，如安全防护，金融认证，考试防作弊等，深入到我们日常生活的诸多方面。
应用四：自然语言处理。
自然语言处理主要研究人类语言如何快速准确地被计算机系统识别和利用的技术。当今自然语言处理被广泛应用在语言翻译，文章主要观点的概括，语音的辨识等方面。
卷积神经网络由于其出色的特征提取和分类能力，如今也被应用在自然语言处理领域中，如人类语言情感的分类，垃圾邮件的分类等。

\subsection{机器学习模型的攻击}
随着机器学习技术在实际应用中的普及，越来越多针对机器学习模型的攻击也随之产生。
纵观针对机器学习模型的攻击方法，大致可以分为三种攻击类型，分别是模型抽取攻击（Model Extraction Attack）、模型逆向攻击（Model Inversion Attack）还有成员推理攻击（Membership Inference Attack）。
模型抽取攻击是指利用机器学习模型的输出数据，进行分析综合，最终获取模型参数的机器学习模型攻击方法。
模型逆向攻击是指利用机器学习模型的输出数据，进行推理分析，最终推断出重要的统计信息的机器学习模型攻击方法。
成员推断攻击是指利用机器学习模型的输出数据，进行加工统计，最终判断某条记录是否被用于训练目标模型的机器学习模型攻击方法。
三种方法相同点和不同点的对比如表~\ref{tab:model-attack-difference}所示。
\begin{table}
    \centering
    \caption{机器学习模型攻击方法的对比}
    \begin{tabular}{lll}
      \toprule
      攻击类型  & 攻击途径 & 攻击目标  \\
      \midrule
      模型抽取攻击   & 利用模型API输出 & 获取模型 \\
      模型逆向攻击   & 利用模型API输出 & 获取统计信息                    \\
      成员推断攻击 & 利用模型API输出  & 判断某记录是够被用于训练模型  \\
      \bottomrule
    \end{tabular}
    \label{tab:model-attack-difference}
\end{table}
\paragraph{}随着越来越多针对机器学习模型攻击的产生，机器学习模型的信息泄露问题应当得到重视。一些用户不希望自己隐私数据被用来训练模型，因此一些已经训练好的模型想办法将这部分用户的数据进行“遗忘”。
\subsection{GDPR等隐私保护法规的出台}
\paragraph{}用户的被遗忘权早在2018年就被欧盟提出，并于2018年5月25日正式生效。《通用数据保护条例（GDPR）》是由欧盟
\subsection{选题的意义}

\section{研究现状和主要挑战}

\section{研究方法}

\section{论文结构安排}

\section{本章小结}

